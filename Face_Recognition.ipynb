{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93f34bbd",
   "metadata": {},
   "source": [
    "# Capstone Project on Real Time Face Recognition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90822bdd",
   "metadata": {},
   "source": [
    "# OBJECTIVE:\n",
    "\n",
    "The objective of creating an OpenCV model for real-time face recognition is to develop a system that can detect and recognize faces in live video streams in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9ab00",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "CNNs and OpenCV are powerful tools for image analysis and computer vision applications. While CNNs excel at learning complex features directly from raw data, OpenCV provides a rich set of functions and algorithms for processing and analyzing images and videos, making them complementary technologies for developing sophisticated computer vision systems.\n",
    "    \n",
    "The combination of Convolutional Neural Networks (CNNs) and OpenCV provides a robust and efficient solution for face recognition tasks. CNNs excel at learning complex facial features, while OpenCV offers tools for face detection, preprocessing, and seamless integration with deep learning frameworks, making them essential components of modern face recognition systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbd4af",
   "metadata": {},
   "source": [
    "# Import Library and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa5203d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cv2\n",
    "import warnings\n",
    "import os\n",
    "from keras.preprocessing.image  import    array_to_img ,   ImageDataGenerator,   img_to_array ,   load_img\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "from keras.models   import Sequential\n",
    "from keras.layers  import Conv2D ,MaxPool2D , Flatten , Dense , Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce181df4",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Data augmentation is a crucial technique used in machine learning, particularly in image data, to increase the diversity of the training dataset by applying various transformations to the existing images. This helps improve the generalization and robustness of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a945196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GenrateImage  = ImageDataGenerator( rotation_range= 40 , width_shift_range=  .2 ,\n",
    "                                     height_shift_range  = .2 , shear_range = .2,\n",
    "                                     zoom_range = .2 ,  horizontal_flip  = True , fill_mode = 'nearest' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f0a78",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5049797",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=load_img(r\"C:\\Users\\LENOVO\\Downloads\\aa.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "612e002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=img_to_array(img)#converting image into numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "739a89a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1280, 720, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=x.reshape((1, ) + x.shape)#reshaping the array to (1, ) + x.shape adds a batch dimension to the input data, making it compatible with deep learning models and ensuring consistency with other data processing operations.\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60651fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch in GenrateImage.flow(x, batch_size = 1, save_to_dir= \"D:\\Prabhat\" , save_prefix= \"pp\", save_format= \"PNG\"):\n",
    "    i = i + 1\n",
    "    if i > 100:\n",
    "        break\n",
    "#to generate augumented image        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e9fcb",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e1a3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = r'D:\\Face'   #This line sets the directory path where the images are stored.\n",
    "cate = ['Arya','Prabhat','Prathamesh']\n",
    "image_sizes = 400\n",
    "input_images= []\n",
    "\n",
    "for i in cate:\n",
    "    folder_path = os.path.join(path2, i)\n",
    "    label = cate.index(i) # we need to tell software which image is of whom\n",
    "    for image_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        image_array = cv2.imread(image_path) # using the cv2 i am reading the image and storing in image array\n",
    "        image_array =cv2.resize(image_array, (image_sizes , image_sizes)) # resizing each image \n",
    "        input_images.append([image_array, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "205bda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y= []\n",
    "\n",
    "for x_values , labels in input_images:\n",
    "    x.append(x_values)\n",
    "    y.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c58a0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "y= np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1fe3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x/255 # to scale the pixel values to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2766e6",
   "metadata": {},
   "source": [
    "# Sampling of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c5b463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9821eb",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a14b133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "8/8 [==============================] - 10s 690ms/step - loss: 1.0921 - accuracy: 0.3083 - val_loss: 0.9945 - val_accuracy: 0.6557\n",
      "Epoch 2/8\n",
      "8/8 [==============================] - 4s 502ms/step - loss: 0.9173 - accuracy: 0.5708 - val_loss: 0.8272 - val_accuracy: 0.7049\n",
      "Epoch 3/8\n",
      "8/8 [==============================] - 4s 490ms/step - loss: 0.5435 - accuracy: 0.8375 - val_loss: 0.4662 - val_accuracy: 0.8197\n",
      "Epoch 4/8\n",
      "8/8 [==============================] - 4s 506ms/step - loss: 0.2192 - accuracy: 0.9042 - val_loss: 0.2371 - val_accuracy: 0.8525\n",
      "Epoch 5/8\n",
      "8/8 [==============================] - 4s 511ms/step - loss: 0.0825 - accuracy: 0.9708 - val_loss: 0.1584 - val_accuracy: 0.9344\n",
      "Epoch 6/8\n",
      "8/8 [==============================] - 4s 478ms/step - loss: 0.0562 - accuracy: 0.9875 - val_loss: 0.1738 - val_accuracy: 0.9508\n",
      "Epoch 7/8\n",
      "8/8 [==============================] - 4s 477ms/step - loss: 0.2993 - accuracy: 0.9500 - val_loss: 0.4967 - val_accuracy: 0.8361\n",
      "Epoch 8/8\n",
      "8/8 [==============================] - 4s 489ms/step - loss: 0.2529 - accuracy: 0.9000 - val_loss: 0.0896 - val_accuracy: 0.9836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f7aed7f1d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(256, activation='relu', input_shape = x.shape[1:]))\n",
    "\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=8, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91d2be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  face_extractor(img):\n",
    "    faces = face_cascade.detectMultiScale(img, scaleFactor = 1.5 , minNeighbors = 5)\n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    for (x ,y , w, h)  in faces:\n",
    "        cv2.rectangle(img , (x,y) , (x+w , y+h), (0,0, 255) , 2)\n",
    "        roi =  img[y :y+h , x:x+w]\n",
    "    \n",
    "    return roi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c8017",
   "metadata": {},
   "source": [
    "haarcascade_frontalface_default.xml contains the trained parameters necessary for detecting frontal faces using the Haar cascade method.\\\n",
    "it can be use to detect faces in images or video frames by applying the detectMultiScale() method.\\\n",
    "This method scans the image or frame using the specified cascade classifier and returns the bounding boxes around detected faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7cf00e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade =  cv2.CascadeClassifier(r\"C:\\Users\\LENOVO\\Downloads\\haarcascade_frontalface_default.xml\")\n",
    "model = load_model(\"face.h5\")  # name of m y model---creted after CNN\n",
    "\n",
    "video_capture =cv2.VideoCapture(0) # front camera\n",
    "while True:\n",
    "    ret,frame = video_capture.read()\n",
    "    \n",
    "    face = face_extractor(frame)\n",
    "    if type(face) is np.ndarray:\n",
    "        face = cv2.resize(face , (400,400)) # based on the way u have build the cnn model\n",
    "        im = Image.fromarray(face , 'RGB')\n",
    "        \n",
    "        \n",
    "        img_array = np.array(im)\n",
    "        \n",
    "        img_array = np.expand_dims(img_array ,  axis = 0)\n",
    "        pred = model.predict(img_array)\n",
    "        print(pred)\n",
    "        \n",
    "        name = \"None Matching\"\n",
    "\n",
    "        if pred[0][0]  > .5:\n",
    "            name =\"Arya\"\n",
    "        elif pred[0][1]  > .5:\n",
    "            name = 'Prabhat'\n",
    "        elif pred[0][2]  > .5:\n",
    "            name = 'Prathamesh'    \n",
    "        cv2.putText(frame ,name , (75 , 75) , cv2.FONT_HERSHEY_COMPLEX , 1 , (255 , 0, 0) , 2)    \n",
    "    else:\n",
    "        cv2.putText(frame ,\"Face not found\" , (75 , 75 ) , cv2.FONT_HERSHEY_COMPLEX ,1 , (255 , 0, 0) , 2)\n",
    "    cv2.imshow('Video' , frame)    \n",
    "        \n",
    "    if cv2.waitKey(20) & 0xff == ord('q') : \n",
    "        break\n",
    "     \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d980e42",
   "metadata": {},
   "source": [
    "# Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7268ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 138ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aefa4db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       0.95      1.00      0.98        20\n",
      "           2       1.00      0.95      0.97        20\n",
      "\n",
      "    accuracy                           0.98        61\n",
      "   macro avg       0.98      0.98      0.98        61\n",
      "weighted avg       0.98      0.98      0.98        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_classes) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba14cc5",
   "metadata": {},
   "source": [
    "**Model shows excellent results in every Performance Parameterics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf497ef",
   "metadata": {},
   "source": [
    "# Key findings\n",
    "The project has highlighted several key findings:\\\n",
    "The effectiveness of deep learning-based approaches, particularly CNNs, in learning discriminative features for face recognition.\\\n",
    "The importance of data preprocessing and augmentation techniques in enhancing the performance and generalization ability of the model.\\\n",
    "The significance of real-time face detection algorithms in enabling timely and efficient recognition of faces in live video streams.\\\n",
    "Furthermore, the successful implementation of the project holds significant practical implications in various domains, including security, surveillance, and human-computer interaction. The ability to accurately identify individuals in real-time video streams can greatly enhance security measures and improve user experience in interactive applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74456c",
   "metadata": {},
   "source": [
    "Despite the achievements of the project, there are still areas for further improvement and research. Future work could focus on:\\\n",
    "Exploring more advanced deep learning architectures and algorithms for face recognition, such as attention mechanisms or siamese networks.\\\n",
    "Investigating techniques for handling variations in lighting, pose, and occlusion to improve the robustness of the model.\\\n",
    "Extending the application scope to include multimodal biometric recognition or emotion detection for more comprehensive human behavior analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e9e71",
   "metadata": {},
   "source": [
    "# conclusion:\n",
    "In conclusion, the development of a real-time face recognition model using OpenCV and CNN has demonstrated promising results in accurately detecting and recognizing faces in live video streams. Through the integration of OpenCV for face detection and CNN for face recognition, we have created a robust system capable of performing real-time face recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa36fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
